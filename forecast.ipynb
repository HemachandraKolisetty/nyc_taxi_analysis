{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d9f2fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f30980a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FareForecastingModel(nn.Module):\n",
    "    def __init__(self, num_locations, embedding_dim, num_numeric_features, lstm_hidden_dim, lstm_layers):\n",
    "        super(FareForecastingModel, self).__init__()\n",
    "        \n",
    "        # Embedding layers for pick-up and drop-off\n",
    "        self.pickup_embedding = nn.Embedding(num_locations, embedding_dim)\n",
    "        self.dropoff_embedding = nn.Embedding(num_locations, embedding_dim)\n",
    "        \n",
    "        # LSTM for modeling temporal sequence\n",
    "        # Suppose our input for LSTM is the concatenation of embeddings + numeric features at each time step\n",
    "        # The input dimension for LSTM: 2*embedding_dim + num_numeric_features\n",
    "        lstm_input_dim = 2 * embedding_dim + num_numeric_features\n",
    "        self.lstm = nn.LSTM(input_size=lstm_input_dim, hidden_size=lstm_hidden_dim, \n",
    "                            num_layers=lstm_layers, batch_first=True)\n",
    "        \n",
    "        # Fully connected layer to produce the forecast\n",
    "        self.fc = nn.Linear(lstm_hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, pickup_ids, dropoff_ids, numeric_seq):\n",
    "        # pickup_ids, dropoff_ids are assumed to have shape (batch_size, seq_length)\n",
    "        # numeric_seq has shape (batch_size, seq_length, num_numeric_features)\n",
    "        \n",
    "        # Get embeddings (result shape: (batch_size, seq_length, embedding_dim))\n",
    "        pickup_emb = self.pickup_embedding(pickup_ids)\n",
    "        dropoff_emb = self.dropoff_embedding(dropoff_ids)\n",
    "        \n",
    "        # Concatenate embeddings with numeric features along last dimension\n",
    "        # New shape: (batch_size, seq_length, 2*embedding_dim + num_numeric_features)\n",
    "        lstm_input = torch.cat((pickup_emb, dropoff_emb, numeric_seq), dim=-1)\n",
    "        \n",
    "        # Pass through LSTM\n",
    "        lstm_out, _ = self.lstm(lstm_input)\n",
    "        # For simplicity, predict using the output at the final time step\n",
    "        final_output = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Forecast output\n",
    "        forecast = self.fc(final_output)\n",
    "        return forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e9db630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding Functions\n",
    "\n",
    "def circular_encoder(df):\n",
    "    # Encode dow and hour as sin and cos\n",
    "    \n",
    "    df['dow_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7.0)\n",
    "    df['dow_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7.0)\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24.0)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24.0)\n",
    "    df.drop(['day_of_week', 'hour'], axis=1, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "976bbd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_amount_base_model(X_train, y_train, X_test, y_test):\n",
    "    # 1) remap your IDs exactly as before\n",
    "    unique_ids = sorted(set(X_train['PULocationID']).union(X_train['DOLocationID']))\n",
    "    id_to_index = {loc_id: idx for idx, loc_id in enumerate(unique_ids)}\n",
    "    for df in (X_train, X_test):\n",
    "        df['PULocationID'] = df['PULocationID'].map(id_to_index)\n",
    "        df['DOLocationID'] = df['DOLocationID'].map(id_to_index)\n",
    "\n",
    "    num_locations = len(unique_ids)\n",
    "    num_numeric  = X_train.shape[1] - 2\n",
    "\n",
    "    # 2) pick your device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # 3) build & move model + loss\n",
    "    model     = FareForecastingModel(\n",
    "        num_locations=num_locations,\n",
    "        embedding_dim=8,\n",
    "        num_numeric_features=num_numeric,\n",
    "        lstm_hidden_dim=64,\n",
    "        lstm_layers=2\n",
    "    ).to(device)\n",
    "    criterion = nn.MSELoss().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    # 4) create four big tensors on GPU once\n",
    "    pu_train  = torch.tensor(X_train['PULocationID'].values).long().to(device)\n",
    "    do_train  = torch.tensor(X_train['DOLocationID'].values).long().to(device)\n",
    "    num_train = torch.tensor(\n",
    "        X_train.drop(['PULocationID','DOLocationID'], axis=1).values\n",
    "    ).float().to(device)\n",
    "    y_train_t = torch.tensor(y_train.values).float().to(device)\n",
    "\n",
    "    # for eval later\n",
    "    pu_test   = torch.tensor(X_test['PULocationID'].values).long().to(device)\n",
    "    do_test   = torch.tensor(X_test['DOLocationID'].values).long().to(device)\n",
    "    num_test  = torch.tensor(\n",
    "        X_test.drop(['PULocationID','DOLocationID'], axis=1).values\n",
    "    ).float().to(device)\n",
    "\n",
    "    # 5) training loop slices those GPU tensors directly\n",
    "    batch_size  = 32\n",
    "    num_samples = pu_train.size(0)\n",
    "    num_batches = num_samples // batch_size\n",
    "\n",
    "    start = time.time()\n",
    "    for epoch in range(10):\n",
    "        print(f\"Starting epoch: {epoch}\")\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i in range(num_batches):\n",
    "            s = i * batch_size\n",
    "            e = s + batch_size\n",
    "\n",
    "            batch_pu   = pu_train[s:e].unsqueeze(1)\n",
    "            batch_do   = do_train[s:e].unsqueeze(1)\n",
    "            batch_nums = num_train[s:e].unsqueeze(1)\n",
    "            batch_y    = y_train_t[s:e]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(batch_pu, batch_do, batch_nums)\n",
    "            loss  = criterion(preds.squeeze(), batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * batch_size\n",
    "\n",
    "        print(f\"Epoch {epoch+1:02d}  avg loss: {running_loss/num_samples:.4f}\")\n",
    "\n",
    "    print(\"Training Time:\", time.time() - start)\n",
    "    return model, (pu_test, do_test, num_test, y_test.values)\n",
    "\n",
    "def evaluate(model, test_tensors):\n",
    "    pu_test, do_test, num_test, y_test_vals = test_tensors\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = model(\n",
    "            pu_test.unsqueeze(1),\n",
    "            do_test.unsqueeze(1),\n",
    "            num_test.unsqueeze(1)\n",
    "        ).squeeze().cpu().numpy()\n",
    "\n",
    "    rmse = ((preds - y_test_vals)**2).mean()**0.5\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    return rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "113119a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "df_train_base = pd.read_csv('data/train.csv')\n",
    "df_test_base = pd.read_csv('data/test.csv')\n",
    "\n",
    "df_train = df_train_base.copy()\n",
    "df_test = df_test_base.copy()\n",
    "\n",
    "df_train = circular_encoder(df_train)\n",
    "df_test = circular_encoder(df_test)\n",
    "\n",
    "X_train= df_train.drop(['travel_time', 'total_amount'], axis=1)\n",
    "y_train = df_train['total_amount']\n",
    "\n",
    "X_test = df_test.drop(['travel_time', 'total_amount'], axis=1)\n",
    "y_test = df_test['total_amount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b293c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def train_arimax(X_train, y_train, X_test, y_test, order=(1, 1, 1)):\n",
    "    \"\"\"\n",
    "    Fits an ARIMA(p,d,q) with exogenous regressors:\n",
    "       y_t = ARIMA-errors + β · X_t\n",
    "    Returns the fitted model and out‑of‑sample forecasts.\n",
    "    \"\"\"\n",
    "    # 1) drop your PU/DO ID columns (or one‑hot / embed them if you really want to use them)\n",
    "    exog_train = X_train.drop(['PULocationID','DOLocationID'], axis=1)\n",
    "    exog_test  = X_test .drop(['PULocationID','DOLocationID'], axis=1)\n",
    "\n",
    "    # 2) build & fit\n",
    "    model = sm.tsa.statespace.SARIMAX(\n",
    "        endog=y_train,\n",
    "        exog=exog_train,\n",
    "        order=order,\n",
    "        enforce_stationarity=False,\n",
    "        enforce_invertibility=False\n",
    "    )\n",
    "    res = model.fit(disp=False)\n",
    "    print(res.summary())\n",
    "\n",
    "    # 3) forecast the next len(y_test) points\n",
    "    forecast = res.get_forecast(\n",
    "        steps=len(y_test),\n",
    "        exog=exog_test\n",
    "    ).predicted_mean\n",
    "\n",
    "    # 4) evaluate\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, forecast))\n",
    "    print(f\"\\nARIMAX{order} RMSE: {rmse:.4f}\")\n",
    "    return res, forecast\n",
    "\n",
    "# --- how you’d call it:\n",
    "# choose (p,d,q) to your taste (you can also use pmdarima.auto_arima to select them)\n",
    "order = (1, 1, 1)\n",
    "\n",
    "arimax_model, arimax_preds = train_arimax(\n",
    "    X_train,       # your DataFrame of regressors\n",
    "    y_train,       # your Series of target fares\n",
    "    X_test,\n",
    "    y_test,\n",
    "    order=order\n",
    ")\n",
    "\n",
    "# ‘arimax_preds’ is a pandas Series aligned 0…len(y_test)-1\n",
    "# if you want an array:\n",
    "arimax_preds = arimax_preds.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d209154b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(df_test_base['PULocationID'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e956ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch: 0\n",
      "Epoch 01  avg loss: 9.5922\n",
      "Starting epoch: 1\n",
      "Epoch 02  avg loss: 9.2465\n",
      "Starting epoch: 2\n",
      "Epoch 03  avg loss: 9.2077\n",
      "Starting epoch: 3\n",
      "Epoch 04  avg loss: 9.1859\n",
      "Starting epoch: 4\n",
      "Epoch 05  avg loss: 9.2075\n",
      "Starting epoch: 5\n",
      "Epoch 06  avg loss: 9.1916\n",
      "Starting epoch: 6\n",
      "Epoch 07  avg loss: 9.1921\n",
      "Starting epoch: 7\n",
      "Epoch 08  avg loss: 9.1967\n",
      "Starting epoch: 8\n",
      "Epoch 09  avg loss: 9.1914\n",
      "Starting epoch: 9\n",
      "Epoch 10  avg loss: 9.1970\n",
      "Training Time: 14530.419337272644\n"
     ]
    }
   ],
   "source": [
    "model = total_amount_base_model(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f69bf43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.FareForecastingModel"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de55ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'fare_forecasting_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24c29424",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8z/knxmpc1d2cv9r49fb2hcyd200000gn/T/ipykernel_57269/1698195209.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('fare_forecasting_model.pth',map_location=torch.device('cpu')))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FareForecastingModel(\n",
       "  (pickup_embedding): Embedding(262, 8)\n",
       "  (dropoff_embedding): Embedding(262, 8)\n",
       "  (lstm): LSTM(24, 64, num_layers=2, batch_first=True)\n",
       "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_ids = sorted(set(X_train['PULocationID']).union(X_train['DOLocationID']))\n",
    "\n",
    "# Load the model\n",
    "model = FareForecastingModel(\n",
    "    num_locations=len(unique_ids),\n",
    "    embedding_dim=8,\n",
    "    num_numeric_features=X_train.shape[1] - 2,\n",
    "    lstm_hidden_dim=64,\n",
    "    lstm_layers=2\n",
    ")\n",
    "model.load_state_dict(torch.load('fare_forecasting_model.pth',map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "\n",
    "device = 'cpu'\n",
    "unique_ids = sorted(set(X_train['PULocationID']).union(X_train['DOLocationID']))\n",
    "id_to_index = {loc_id: idx for idx, loc_id in enumerate(unique_ids)}\n",
    "for df in (X_train, X_test):\n",
    "    df['PULocationID'] = df['PULocationID'].map(id_to_index)\n",
    "    df['DOLocationID'] = df['DOLocationID'].map(id_to_index)\n",
    "pu_test   = torch.tensor(X_test['PULocationID'].values).long().to(device)\n",
    "do_test   = torch.tensor(X_test['DOLocationID'].values).long().to(device)\n",
    "num_test  = torch.tensor(\n",
    "    X_test.drop(['PULocationID','DOLocationID'], axis=1).values\n",
    ").float().to(device)\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcc0fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, (pu_test, do_test, num_test, y_test.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d006ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example instantiation:\n",
    "num_locations = 265\n",
    "embedding_dim = 8\n",
    "num_numeric_features = 5\n",
    "lstm_hidden_dim = 64\n",
    "lstm_layers = 2\n",
    "\n",
    "model = FareForecastingModel(num_locations, embedding_dim, num_numeric_features, lstm_hidden_dim, lstm_layers)\n",
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_proj4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
