{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation started (calculation_id=decb17b4-8073-5433-aa1b-262bbfb0b28b) in (session=1acb17b4-7243-8e54-447a-958e0328119a). Checking calculation status...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ca05c3c675e47fbb9f9df1d71861cd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          |elapsed time = 00:00s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation completed.\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation started (calculation_id=40cb16b8-ac4f-2fb4-0ece-5d7e281df11d) in (session=1ecb16b1-e1a9-baa7-f760-e157ed47fab0). Checking calculation status...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5d89badd1414ec496a0c2e4887ed275",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          |elapsed time = 00:00s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation completed.\n",
      "['VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', 'RatecodeID', 'store_and_fwd_flag', 'PULocationID', 'DOLocationID', 'payment_type', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount', 'congestion_surcharge', 'airport_fee']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_path  = \"s3://cse6242-nyc-trip/yellow_tripdata_202*.parquet\"\n",
    "\n",
    "spark.conf.set(\"spark.sql.parquet.enableVectorizedReader\", \"false\")\n",
    "\n",
    "test_df = spark.read.parquet(input_path)\n",
    "\n",
    "df = df.withColumn(\"PULocationID\", col(\"PULocationID\").cast(\"int\")) \\\n",
    "                 .withColumn(\"DOLocationID\", col(\"DOLocationID\").cast(\"int\"))\n",
    "\n",
    "columns = df.columns\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation started (calculation_id=aecb17c3-10f3-7541-3ef3-999f821554f5) in (session=1acb17b4-7243-8e54-447a-958e0328119a). Checking calculation status...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0c2f4b406bd4383b929d2b5bac6568f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          |elapsed time = 00:00s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation completed.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, year, month, dayofweek, count, sum, avg\n",
    "\n",
    "# Set Spark configurations.\n",
    "spark.conf.set(\"spark.sql.parquet.enableVectorizedReader\", \"false\")\n",
    "spark.conf.set(\"spark.sql.files.ignoreCorruptFiles\", \"true\")\n",
    "spark.conf.set(\"spark.sql.parquet.mergeSchema\", \"false\")\n",
    "\n",
    "# Read the Parquet files from S3.\n",
    "input_path = \"s3://cse6242-nyc-trip/yellow_tripdata_2024-*.parquet\"\n",
    "df = spark.read.parquet(input_path)\n",
    "\n",
    "# Ensure consistent types by casting location IDs.\n",
    "df = df.withColumn(\"PULocationID\", col(\"PULocationID\").cast(\"int\")) \\\n",
    "       .withColumn(\"DOLocationID\", col(\"DOLocationID\").cast(\"int\"))\n",
    "\n",
    "# Optionally cast fare and tip amounts if needed.\n",
    "# df = df.withColumn(\"fare_amount\", col(\"fare_amount\").cast(\"double\")) \\\n",
    "#        .withColumn(\"tip_amount\", col(\"tip_amount\").cast(\"double\"))\n",
    "\n",
    "# 1. Aggregate trips by year and pickup location.\n",
    "pickup_year_df = df.withColumn(\"year\", year(col(\"tpep_pickup_datetime\"))) \\\n",
    "    .groupBy(\"year\", \"PULocationID\") \\\n",
    "    .agg( count(\"*\").alias(\"trip_count\"),\n",
    "          sum(\"fare_amount\").alias(\"total_fare\"),\n",
    "          sum(\"tip_amount\").alias(\"total_tip\"),\n",
    "          avg(\"fare_amount\").alias(\"avg_fare\"),\n",
    "          avg(\"tip_amount\").alias(\"avg_tip\") )\n",
    "\n",
    "# 2. Aggregate trips by year and dropoff location.\n",
    "dropoff_year_df = df.withColumn(\"year\", year(col(\"tpep_dropoff_datetime\"))) \\\n",
    "    .groupBy(\"year\", \"DOLocationID\") \\\n",
    "    .agg( count(\"*\").alias(\"trip_count\"),\n",
    "          sum(\"fare_amount\").alias(\"total_fare\"),\n",
    "          sum(\"tip_amount\").alias(\"total_tip\"),\n",
    "          avg(\"fare_amount\").alias(\"avg_fare\"),\n",
    "          avg(\"tip_amount\").alias(\"avg_tip\") )\n",
    "\n",
    "# 3. Aggregate trips by year, month, and pickup location.\n",
    "pickup_year_month_df = df.withColumn(\"year\", year(col(\"tpep_pickup_datetime\"))) \\\n",
    "    .withColumn(\"month\", month(col(\"tpep_pickup_datetime\"))) \\\n",
    "    .groupBy(\"year\", \"month\", \"PULocationID\") \\\n",
    "    .agg( count(\"*\").alias(\"trip_count\"),\n",
    "          sum(\"fare_amount\").alias(\"total_fare\"),\n",
    "          sum(\"tip_amount\").alias(\"total_tip\"),\n",
    "          avg(\"fare_amount\").alias(\"avg_fare\"),\n",
    "          avg(\"tip_amount\").alias(\"avg_tip\") )\n",
    "\n",
    "# 4. Aggregate trips by year, month, and dropoff location.\n",
    "dropoff_year_month_df = df.withColumn(\"year\", year(col(\"tpep_dropoff_datetime\"))) \\\n",
    "    .withColumn(\"month\", month(col(\"tpep_dropoff_datetime\"))) \\\n",
    "    .groupBy(\"year\", \"month\", \"DOLocationID\") \\\n",
    "    .agg( count(\"*\").alias(\"trip_count\"),\n",
    "          sum(\"fare_amount\").alias(\"total_fare\"),\n",
    "          sum(\"tip_amount\").alias(\"total_tip\"),\n",
    "          avg(\"fare_amount\").alias(\"avg_fare\"),\n",
    "          avg(\"tip_amount\").alias(\"avg_tip\") )\n",
    "\n",
    "# 5. Aggregate trips by year, month, day-of-week, and pickup location.\n",
    "pickup_year_month_day_df = df.withColumn(\"year\", year(col(\"tpep_pickup_datetime\"))) \\\n",
    "    .withColumn(\"month\", month(col(\"tpep_pickup_datetime\"))) \\\n",
    "    .withColumn(\"dayOfWeek\", dayofweek(col(\"tpep_pickup_datetime\"))) \\\n",
    "    .groupBy(\"year\", \"month\", \"dayOfWeek\", \"PULocationID\") \\\n",
    "    .agg( count(\"*\").alias(\"trip_count\"),\n",
    "          sum(\"fare_amount\").alias(\"total_fare\"),\n",
    "          sum(\"tip_amount\").alias(\"total_tip\"),\n",
    "          avg(\"fare_amount\").alias(\"avg_fare\"),\n",
    "          avg(\"tip_amount\").alias(\"avg_tip\") )\n",
    "\n",
    "# 6. Aggregate trips by year, month, day-of-week, and dropoff location.\n",
    "dropoff_year_month_day_df = df.withColumn(\"year\", year(col(\"tpep_dropoff_datetime\"))) \\\n",
    "    .withColumn(\"month\", month(col(\"tpep_dropoff_datetime\"))) \\\n",
    "    .withColumn(\"dayOfWeek\", dayofweek(col(\"tpep_dropoff_datetime\"))) \\\n",
    "    .groupBy(\"year\", \"month\", \"dayOfWeek\", \"DOLocationID\") \\\n",
    "    .agg( count(\"*\").alias(\"trip_count\"),\n",
    "          sum(\"fare_amount\").alias(\"total_fare\"),\n",
    "          sum(\"tip_amount\").alias(\"total_tip\"),\n",
    "          avg(\"fare_amount\").alias(\"avg_fare\"),\n",
    "          avg(\"tip_amount\").alias(\"avg_tip\") )\n",
    "\n",
    "# Define the output S3 paths (update these paths as required)\n",
    "pickup_year_output_path            = \"s3://cse6242-nyc-trip/pickup_year_2024.csv\"\n",
    "dropoff_year_output_path           = \"s3://cse6242-nyc-trip/dropoff_year_2024.csv\"\n",
    "pickup_year_month_output_path      = \"s3://cse6242-nyc-trip/pickup_year_month_2024.csv\"\n",
    "dropoff_year_month_output_path     = \"s3://cse6242-nyc-trip/dropoff_year_month_2024.csv\"\n",
    "pickup_year_month_day_output_path  = \"s3://cse6242-nyc-trip/pickup_year_month_day_2024.csv\"\n",
    "dropoff_year_month_day_output_path = \"s3://cse6242-nyc-trip/dropoff_year_month_day_2024.csv\"\n",
    "\n",
    "# Write the results as CSV files with headers. (No overwrite mode is specified.)\n",
    "pickup_year_df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(pickup_year_output_path)\n",
    "dropoff_year_df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(dropoff_year_output_path)\n",
    "pickup_year_month_df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(pickup_year_month_output_path)\n",
    "dropoff_year_month_df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(dropoff_year_month_output_path)\n",
    "pickup_year_month_day_df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(pickup_year_month_day_output_path)\n",
    "dropoff_year_month_day_df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(dropoff_year_month_day_output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation started (calculation_id=becb17c3-7709-248f-f4c5-c3c843e2ade1) in (session=1acb17b4-7243-8e54-447a-958e0328119a). Checking calculation status...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90d93750711a4756b533db4d1f7b9ad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          |elapsed time = 00:00s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation completed.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, year, month, dayofweek, count, sum, avg\n",
    "\n",
    "# Set Spark configurations.\n",
    "spark.conf.set(\"spark.sql.parquet.enableVectorizedReader\", \"false\")\n",
    "spark.conf.set(\"spark.sql.files.ignoreCorruptFiles\", \"true\")\n",
    "spark.conf.set(\"spark.sql.parquet.mergeSchema\", \"false\")\n",
    "\n",
    "# Read the Parquet files from S3.\n",
    "input_path = \"s3://cse6242-nyc-trip/yellow_tripdata_2023-*.parquet\"\n",
    "df = spark.read.parquet(input_path)\n",
    "\n",
    "# Ensure consistent types by casting location IDs.\n",
    "df = df.withColumn(\"PULocationID\", col(\"PULocationID\").cast(\"int\")) \\\n",
    "       .withColumn(\"DOLocationID\", col(\"DOLocationID\").cast(\"int\"))\n",
    "\n",
    "# Optionally cast fare and tip amounts if needed.\n",
    "# df = df.withColumn(\"fare_amount\", col(\"fare_amount\").cast(\"double\")) \\\n",
    "#        .withColumn(\"tip_amount\", col(\"tip_amount\").cast(\"double\"))\n",
    "\n",
    "# 1. Aggregate trips by year and pickup location.\n",
    "pickup_year_df = df.withColumn(\"year\", year(col(\"tpep_pickup_datetime\"))) \\\n",
    "    .groupBy(\"year\", \"PULocationID\") \\\n",
    "    .agg( count(\"*\").alias(\"trip_count\"),\n",
    "          sum(\"fare_amount\").alias(\"total_fare\"),\n",
    "          sum(\"tip_amount\").alias(\"total_tip\"),\n",
    "          avg(\"fare_amount\").alias(\"avg_fare\"),\n",
    "          avg(\"tip_amount\").alias(\"avg_tip\") )\n",
    "\n",
    "# 2. Aggregate trips by year and dropoff location.\n",
    "dropoff_year_df = df.withColumn(\"year\", year(col(\"tpep_dropoff_datetime\"))) \\\n",
    "    .groupBy(\"year\", \"DOLocationID\") \\\n",
    "    .agg( count(\"*\").alias(\"trip_count\"),\n",
    "          sum(\"fare_amount\").alias(\"total_fare\"),\n",
    "          sum(\"tip_amount\").alias(\"total_tip\"),\n",
    "          avg(\"fare_amount\").alias(\"avg_fare\"),\n",
    "          avg(\"tip_amount\").alias(\"avg_tip\") )\n",
    "\n",
    "# 3. Aggregate trips by year, month, and pickup location.\n",
    "pickup_year_month_df = df.withColumn(\"year\", year(col(\"tpep_pickup_datetime\"))) \\\n",
    "    .withColumn(\"month\", month(col(\"tpep_pickup_datetime\"))) \\\n",
    "    .groupBy(\"year\", \"month\", \"PULocationID\") \\\n",
    "    .agg( count(\"*\").alias(\"trip_count\"),\n",
    "          sum(\"fare_amount\").alias(\"total_fare\"),\n",
    "          sum(\"tip_amount\").alias(\"total_tip\"),\n",
    "          avg(\"fare_amount\").alias(\"avg_fare\"),\n",
    "          avg(\"tip_amount\").alias(\"avg_tip\") )\n",
    "\n",
    "# 4. Aggregate trips by year, month, and dropoff location.\n",
    "dropoff_year_month_df = df.withColumn(\"year\", year(col(\"tpep_dropoff_datetime\"))) \\\n",
    "    .withColumn(\"month\", month(col(\"tpep_dropoff_datetime\"))) \\\n",
    "    .groupBy(\"year\", \"month\", \"DOLocationID\") \\\n",
    "    .agg( count(\"*\").alias(\"trip_count\"),\n",
    "          sum(\"fare_amount\").alias(\"total_fare\"),\n",
    "          sum(\"tip_amount\").alias(\"total_tip\"),\n",
    "          avg(\"fare_amount\").alias(\"avg_fare\"),\n",
    "          avg(\"tip_amount\").alias(\"avg_tip\") )\n",
    "\n",
    "# 5. Aggregate trips by year, month, day-of-week, and pickup location.\n",
    "pickup_year_month_day_df = df.withColumn(\"year\", year(col(\"tpep_pickup_datetime\"))) \\\n",
    "    .withColumn(\"month\", month(col(\"tpep_pickup_datetime\"))) \\\n",
    "    .withColumn(\"dayOfWeek\", dayofweek(col(\"tpep_pickup_datetime\"))) \\\n",
    "    .groupBy(\"year\", \"month\", \"dayOfWeek\", \"PULocationID\") \\\n",
    "    .agg( count(\"*\").alias(\"trip_count\"),\n",
    "          sum(\"fare_amount\").alias(\"total_fare\"),\n",
    "          sum(\"tip_amount\").alias(\"total_tip\"),\n",
    "          avg(\"fare_amount\").alias(\"avg_fare\"),\n",
    "          avg(\"tip_amount\").alias(\"avg_tip\") )\n",
    "\n",
    "# 6. Aggregate trips by year, month, day-of-week, and dropoff location.\n",
    "dropoff_year_month_day_df = df.withColumn(\"year\", year(col(\"tpep_dropoff_datetime\"))) \\\n",
    "    .withColumn(\"month\", month(col(\"tpep_dropoff_datetime\"))) \\\n",
    "    .withColumn(\"dayOfWeek\", dayofweek(col(\"tpep_dropoff_datetime\"))) \\\n",
    "    .groupBy(\"year\", \"month\", \"dayOfWeek\", \"DOLocationID\") \\\n",
    "    .agg( count(\"*\").alias(\"trip_count\"),\n",
    "          sum(\"fare_amount\").alias(\"total_fare\"),\n",
    "          sum(\"tip_amount\").alias(\"total_tip\"),\n",
    "          avg(\"fare_amount\").alias(\"avg_fare\"),\n",
    "          avg(\"tip_amount\").alias(\"avg_tip\") )\n",
    "\n",
    "# Define the output S3 paths (update these paths as required)\n",
    "pickup_year_output_path            = \"s3://cse6242-nyc-trip/pickup_year_2023.csv\"\n",
    "dropoff_year_output_path           = \"s3://cse6242-nyc-trip/dropoff_year_2023.csv\"\n",
    "pickup_year_month_output_path      = \"s3://cse6242-nyc-trip/pickup_year_month_2023.csv\"\n",
    "dropoff_year_month_output_path     = \"s3://cse6242-nyc-trip/dropoff_year_month_2023.csv\"\n",
    "pickup_year_month_day_output_path  = \"s3://cse6242-nyc-trip/pickup_year_month_day_2023.csv\"\n",
    "dropoff_year_month_day_output_path = \"s3://cse6242-nyc-trip/dropoff_year_month_day_2023.csv\"\n",
    "\n",
    "# Write the results as CSV files with headers. (No overwrite mode is specified.)\n",
    "pickup_year_df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(pickup_year_output_path)\n",
    "dropoff_year_df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(dropoff_year_output_path)\n",
    "pickup_year_month_df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(pickup_year_month_output_path)\n",
    "dropoff_year_month_df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(dropoff_year_month_output_path)\n",
    "pickup_year_month_day_df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(pickup_year_month_day_output_path)\n",
    "dropoff_year_month_day_df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(dropoff_year_month_day_output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation started (calculation_id=88cb1653-08d1-a50e-52c2-8ae533cacdbb) in (session=f2cb1650-a0d1-8460-fab5-ce0c232c7978). Checking calculation status...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fcfad3a02ec48be88c7bc6035cd3071",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          |elapsed time = 00:00s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation completed.\n",
      "+------------+------------+-----------+\n",
      "|PULocationID|DOLocationID|fare_amount|\n",
      "+------------+------------+-----------+\n",
      "|         161|         141|        9.3|\n",
      "|          43|         237|        7.9|\n",
      "|          48|         238|       14.9|\n",
      "|         138|           7|       12.1|\n",
      "|         107|          79|       11.4|\n",
      "|         161|         137|       12.8|\n",
      "|         239|         143|       12.1|\n",
      "|         142|         200|       45.7|\n",
      "|         164|         236|       17.7|\n",
      "|         141|         107|       14.9|\n",
      "|         234|          68|       11.4|\n",
      "|          79|         264|       33.8|\n",
      "|         164|         143|       26.1|\n",
      "|         138|          33|       44.3|\n",
      "|          33|          61|       17.7|\n",
      "|          79|         186|       10.0|\n",
      "|          90|          48|       19.8|\n",
      "|         113|         255|       20.5|\n",
      "|         237|         239|        8.6|\n",
      "|         143|         229|       15.6|\n",
      "+------------+------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_df = df.select(\"PULocationID\", \"DOLocationID\", \"fare_amount\" )\n",
    "\n",
    "filtered_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation started (calculation_id=8ecb165a-4980-6ee7-8ba4-7253f569b4d3) in (session=f2cb1650-a0d1-8460-fab5-ce0c232c7978). Checking calculation status...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6250e1f604a546289ca5ee35286755a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          |elapsed time = 00:00s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation completed.\n",
      "79479946\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count = df.count()\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation started (calculation_id=c6caf1cd-d9d7-0fcc-eb4a-ee60f8f5c687) in (session=50caf1c8-00d9-77a0-ed03-8970b33022d7). Checking calculation status...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "562168d5dc514ce6bf15b8b3f64ae14e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          |elapsed time = 00:00s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation completed.\n",
      "+----------+-------------+--------------------+------------+\n",
      "|       _c0|          _c1|                 _c2|         _c3|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "|         6|Staten Island|Arrochar/Fort Wad...|   Boro Zone|\n",
      "|         7|       Queens|             Astoria|   Boro Zone|\n",
      "|         8|       Queens|        Astoria Park|   Boro Zone|\n",
      "|         9|       Queens|          Auburndale|   Boro Zone|\n",
      "|        10|       Queens|        Baisley Park|   Boro Zone|\n",
      "|        11|     Brooklyn|          Bath Beach|   Boro Zone|\n",
      "|        12|    Manhattan|        Battery Park| Yellow Zone|\n",
      "|        13|    Manhattan|   Battery Park City| Yellow Zone|\n",
      "|        14|     Brooklyn|           Bay Ridge|   Boro Zone|\n",
      "|        15|       Queens|Bay Terrace/Fort ...|   Boro Zone|\n",
      "|        16|       Queens|             Bayside|   Boro Zone|\n",
      "|        17|     Brooklyn|             Bedford|   Boro Zone|\n",
      "|        18|        Bronx|        Bedford Park|   Boro Zone|\n",
      "|        19|       Queens|           Bellerose|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "coordinate_lookup = spark.read.csv(\"s3://cse6242-nyc-trip/taxi_zone_lookup.csv\")\n",
    "coordinate_lookup.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation started (calculation_id=16caf1d4-006d-e853-1f45-60a062ec6ca3) in (session=50caf1c8-00d9-77a0-ed03-8970b33022d7). Checking calculation status...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "176a99c57dcd49869cb719d981eb71f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          |elapsed time = 00:00s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation completed.\n"
     ]
    }
   ],
   "source": [
    "# Calculate tip percentage safely by checking fare_amount != 0 to avoid division by zero.\n",
    "df_with_tip_pct = df.withColumn(\n",
    "    \"tip_pct\", \n",
    "    when(col(\"fare_amount\") != 0, col(\"tip_amount\") / col(\"fare_amount\") * 100).otherwise(0)\n",
    ")\n",
    "\n",
    "# Group by pickup and dropoff location IDs and compute average tip percentage\n",
    "avg_tip_df = df_with_tip_pct.groupBy(\"PULocationID\", \"DOLocationID\") \\\n",
    "    .agg(avg(\"tip_pct\").alias(\"avg_tip_pct\"))\n",
    "\n",
    "# Sort by average tip percentage in descending order\n",
    "sorted_tip_df = avg_tip_df.orderBy(desc(\"avg_tip_pct\"))\n",
    "\n",
    "# Define output path (modify accordingly)\n",
    "output_path = \"s3://cse6242-nyc-trip/nyc_avg_tip_pct.csv\"\n",
    "\n",
    "# Write to CSV (ensure it's a single file)\n",
    "sorted_tip_df.coalesce(1).write.csv(output_path, header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation started (calculation_id=9acaf1b1-4942-d448-f402-b2b04d7f7972) in (session=44caf1ab-b63d-2a66-7f18-6a0956d04f39). Checking calculation status...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ff93cb73af9435ca1f6de7556fdb344",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          |elapsed time = 00:00s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation 9acaf1b1-4942-d448-f402-b2b04d7f7972 failed\n",
      "\n",
      "\n",
      "  File \"<stdin>\", line 1, in <module>\n",
      "  File \"/opt/amazon/python3.9/lib/python3.9/site-packages/pandas/core/frame.py\", line 9351, in merge\n",
      "    return merge(\n",
      "  File \"/opt/amazon/python3.9/lib/python3.9/site-packages/pandas/core/reshape/merge.py\", line 107, in merge\n",
      "    op = _MergeOperation(\n",
      "  File \"/opt/amazon/python3.9/lib/python3.9/site-packages/pandas/core/reshape/merge.py\", line 629, in __init__\n",
      "    _right = _validate_operand(right)\n",
      "  File \"/opt/amazon/python3.9/lib/python3.9/site-packages/pandas/core/reshape/merge.py\", line 2285, in _validate_operand\n",
      "    raise TypeError(\n",
      "TypeError: Can only merge Series or DataFrame objects, a <class 'pyspark.sql.dataframe.DataFrame'> was passed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Merge to assign neighborhood names to the pickup locations\n",
    "avg_tip_zones = avg_tip_pd.merge(coordinate_lookup, left_on=\"PULocationID\", right_on=\"LocationID\", how=\"left\")\n",
    "\n",
    "# Further aggregate by neighborhood (Zone) to get the average tip percentage per neighborhood\n",
    "avg_tip_by_zone = avg_tip_zones.groupby(\"Zone\", as_index=False)[\"avg_tip_percentage\"].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation started (calculation_id=1ecaf1b5-36ae-6a1e-45cf-4db1df778c0b) in (session=44caf1ab-b63d-2a66-7f18-6a0956d04f39). Checking calculation status...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30bcb710bea249b58f942e572772c333",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          |elapsed time = 00:00s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation 1ecaf1b5-36ae-6a1e-45cf-4db1df778c0b failed\n",
      "\n",
      "\n",
      "  File \"<stdin>\", line 1, in <module>\n",
      "  File \"/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/conf.py\", line 36, in set\n",
      "    self._jconf.setAthenaRestricted(key, value)\n",
      "  File \"/opt/amazon/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 117, in deco\n",
      "    raise converted from None\n",
      "pyspark.sql.utils.AnalysisException: Cannot modify the value of a Spark config: spark.driver.maxResultSize\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert the Spark DataFrame to Pandas.\n",
    "# Warning: Ensure that the data size is manageable in memory or sample a subset.\n",
    "pdf = df.toPandas()\n",
    "\n",
    "# -------------------------------\n",
    "# Step 2: Prepare Time-related Columns\n",
    "# -------------------------------\n",
    "# Ensure the pickup datetime column is in datetime format.\n",
    "pdf['tpep_pickup_datetime'] = pd.to_datetime(pdf['tpep_pickup_datetime'])\n",
    "\n",
    "# Create additional time-related columns.\n",
    "pdf['date'] = pdf['tpep_pickup_datetime'].dt.date\n",
    "pdf['week'] = pdf['tpep_pickup_datetime'].dt.isocalendar().week\n",
    "pdf['day_of_week'] = pdf['tpep_pickup_datetime'].dt.weekday  # Monday=0, Sunday=6\n",
    "pdf['hour'] = pdf['tpep_pickup_datetime'].dt.hour\n",
    "pdf['month'] = pdf['tpep_pickup_datetime'].dt.month\n",
    "pdf['day'] = pdf['tpep_pickup_datetime'].dt.day\n",
    "\n",
    "# -------------------------------\n",
    "# Step 3: Daily Calendar Heatmap (Week vs. Day)\n",
    "# -------------------------------\n",
    "# Aggregate taxi counts by ISO week and day-of-week.\n",
    "daily_counts = pdf.groupby(['week', 'day_of_week']).size().reset_index(name='count')\n",
    "daily_pivot = daily_counts.pivot(index='week', columns='day_of_week', values='count').fillna(0)\n",
    "\n",
    "# Define labels for days of the week.\n",
    "day_labels = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "\n",
    "fig_daily = px.imshow(\n",
    "    daily_pivot,\n",
    "    labels=dict(x=\"Day of Week\", y=\"ISO Week\", color=\"Trip Count\"),\n",
    "    x=day_labels,\n",
    "    title=\"Daily Taxi Activity (Calendar Heatmap: Week vs Day)\"\n",
    ")\n",
    "fig_daily.update_xaxes(side=\"top\")\n",
    "fig_daily.show()\n",
    "\n",
    "# -------------------------------\n",
    "# Step 4: Weekly Heatmap (Day vs. Hour)\n",
    "# -------------------------------\n",
    "# Aggregate taxi counts by day-of-week and hour.\n",
    "dow_hour = pdf.groupby(['day_of_week', 'hour']).size().reset_index(name='count')\n",
    "dow_hour_pivot = dow_hour.pivot(index='day_of_week', columns='hour', values='count').fillna(0)\n",
    "\n",
    "# Set the row index labels to day names.\n",
    "dow_hour_pivot.index = day_labels\n",
    "\n",
    "fig_weekly = px.imshow(\n",
    "    dow_hour_pivot,\n",
    "    labels=dict(x=\"Hour of Day\", y=\"Day of Week\", color=\"Trip Count\"),\n",
    "    title=\"Weekly Taxi Activity (Day vs Hour Heatmap)\"\n",
    ")\n",
    "fig_weekly.update_xaxes(side=\"top\")\n",
    "fig_weekly.show()\n",
    "\n",
    "# -------------------------------\n",
    "# Step 5: Monthly Calendar Heatmap (Month vs. Day)\n",
    "# -------------------------------\n",
    "# Aggregate taxi counts by month and day-of-month.\n",
    "monthly_counts = pdf.groupby(['month', 'day']).size().reset_index(name='count')\n",
    "monthly_pivot = monthly_counts.pivot(index='month', columns='day', values='count').fillna(0)\n",
    "\n",
    "# Create a list of month abbreviations.\n",
    "month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
    "               'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "# Replace numeric month index with month names.\n",
    "monthly_pivot.index = [month_names[i-1] for i in monthly_pivot.index]\n",
    "\n",
    "fig_monthly = px.imshow(\n",
    "    monthly_pivot,\n",
    "    labels=dict(x=\"Day of Month\", y=\"Month\", color=\"Trip Count\"),\n",
    "    title=\"Monthly Taxi Activity (Month vs Day Heatmap)\"\n",
    ")\n",
    "fig_monthly.update_xaxes(side=\"top\")\n",
    "fig_monthly.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation started (calculation_id=dccaf1bd-9a69-52fb-e8d6-4b4d0be046ac) in (session=44caf1ab-b63d-2a66-7f18-6a0956d04f39). Checking calculation status...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "547bc28edfe54268be60a73aa1267f79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          |elapsed time = 00:00s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation dccaf1bd-9a69-52fb-e8d6-4b4d0be046ac failed\n",
      "\n",
      "\n",
      "  File \"<stdin>\", line 12, in <module>\n",
      "  File \"/opt/amazon/python3.9/lib/python3.9/site-packages/pandas/util/_decorators.py\", line 311, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/amazon/python3.9/lib/python3.9/site-packages/pandas/core/reshape/concat.py\", line 347, in concat\n",
      "    op = _Concatenator(\n",
      "  File \"/opt/amazon/python3.9/lib/python3.9/site-packages/pandas/core/reshape/concat.py\", line 404, in __init__\n",
      "    raise ValueError(\"No objects to concatenate\")\n",
      "ValueError: No objects to concatenate\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Install necessary libraries if not already installed\n",
    "# !pip install pandas seaborn matplotlib pyarrow\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import glob  # To read multiple parquet files\n",
    "\n",
    "# Set plot style\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Step 1: Load all Parquet files from the Dataset folder\n",
    "parquet_files = glob.glob(\"s3://cse6242-nyc-trip/*.parquet\")  # List all parquet files\n",
    "df_list = [pd.read_parquet(file) for file in parquet_files]  # Read each parquet file into DataFrame\n",
    "df = pd.concat(df_list, ignore_index=True)  # Concatenate all DataFrames\n",
    "\n",
    "# Step 2: Load zone.csv\n",
    "zone_df = pd.read_csv(\"s3://cse6242-nyc-trip/taxi_zones_geocoded.csv\")\n",
    "\n",
    "\n",
    "df['trip_duration_min'] = (df['tpep_dropoff_datetime'] - df['tpep_pickup_datetime']).dt.total_seconds() / 60\n",
    "\n",
    "# Step 3: Merge the data if there is a common column (assuming 'zone_id' is the common key)\n",
    "df = df.merge(zone_df, left_on=\"PULocationID\", right_on=\"LocationID\", how=\"left\")\n",
    "\n",
    "heatmap_data = df.pivot_table(index=\"PULocationID\", columns=\"LocationID\", values=\"trip_duration_min\", aggfunc=\"mean\")\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(heatmap_data, cmap=\"coolwarm\", annot=False, linewidths=0.5)\n",
    "\n",
    "# Step 5: Show the heatmap\n",
    "plt.title(\"Heatmap of Average Trip Duration by Pickup Location\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation started (calculation_id=b6caf222-244b-938b-f08e-593b9f628991) in (session=02caf221-e38c-2aa8-ab55-11c30d59e494). Checking calculation status...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8526e85c6f9f4a2e84696ddc3cd6f9c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          |elapsed time = 00:00s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation completed.\n"
     ]
    }
   ],
   "source": [
    "# Extract date, month, and hour for aggregation\n",
    "df_with_time = df.withColumn(\"pickup_date\", to_date(col(\"tpep_pickup_datetime\"))) \\\n",
    "                 .withColumn(\"pickup_month\", date_format(col(\"tpep_pickup_datetime\"), \"yyyy-MM\")) \\\n",
    "                 .withColumn(\"pickup_hour\", hour(col(\"tpep_pickup_datetime\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation started (calculation_id=b4caf222-5336-defa-469b-823ce63b3fe3) in (session=02caf221-e38c-2aa8-ab55-11c30d59e494). Checking calculation status...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a0a9ace3d384650826818f9112d0834",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          |elapsed time = 00:00s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation completed.\n"
     ]
    }
   ],
   "source": [
    "daily_activity = df_with_time.groupBy(\"pickup_date\").agg(count(\"*\").alias(\"trip_count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation started (calculation_id=cecaf222-6f6b-b2c6-7a22-0635fb0ed3cf) in (session=02caf221-e38c-2aa8-ab55-11c30d59e494). Checking calculation status...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed72f9e45a684d138aeed91556f9b785",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          |elapsed time = 00:00s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation completed.\n"
     ]
    }
   ],
   "source": [
    "monthly_activity = df_with_time.groupBy(\"pickup_month\").agg(count(\"*\").alias(\"trip_count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation started (calculation_id=e8caf222-8484-3a26-4c8d-f777bd9facf8) in (session=02caf221-e38c-2aa8-ab55-11c30d59e494). Checking calculation status...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da552e11603a45f0b10317bd8a66df62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          |elapsed time = 00:00s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation completed.\n"
     ]
    }
   ],
   "source": [
    "hourly_activity = df_with_time.groupBy(\"pickup_hour\").agg(count(\"*\").alias(\"trip_count\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation started (calculation_id=cecaf223-2c4e-d8d7-3959-9779674f0922) in (session=02caf221-e38c-2aa8-ab55-11c30d59e494). Checking calculation status...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "862c04fc2bd9487d8602a16c76b1e58a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          |elapsed time = 00:00s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation completed.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, to_date, date_format, hour, count\n",
    "# Define output paths\n",
    "daily_output_path = \"s3://cse6242-nyc-trip/nyc_daily_activity.csv\"\n",
    "monthly_output_path = \"s3://cse6242-nyc-trip/nyc_monthly_activity.csv\"\n",
    "hourly_output_path = \"s3://cse6242-nyc-trip/nyc_hourly_activity.csv\"\n",
    "\n",
    "# Save to CSV with headers\n",
    "daily_activity.coalesce(1).write.csv(daily_output_path, header=True, mode=\"overwrite\")\n",
    "monthly_activity.coalesce(1).write.csv(monthly_output_path, header=True, mode=\"overwrite\")\n",
    "hourly_activity.coalesce(1).write.csv(hourly_output_path, header=True, mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load daily data\n",
    "daily_df = pd.read_csv(\"nyc_daily_activity.csv\", parse_dates=[\"pickup_date\"])\n",
    "daily_df[\"day\"] = daily_df[\"pickup_date\"].dt.day\n",
    "daily_df[\"month\"] = daily_df[\"pickup_date\"].dt.month_name()\n",
    "\n",
    "# Pivot for heatmap (daily)\n",
    "daily_pivot = daily_df.pivot(\"month\", \"day\", \"trip_count\")\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(daily_pivot, cmap=\"YlGnBu\", annot=False)\n",
    "plt.title(\"Daily Taxi Activity Heatmap\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation started (calculation_id=f0cb169a-914f-6c0b-086f-f91db4f417f9) in (session=40cb1698-5db5-4b54-871f-bb5c6f2edb87). Checking calculation status...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "341c13ec192e45b1bfe73e50433e9c73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          |elapsed time = 00:00s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation f0cb169a-914f-6c0b-086f-f91db4f417f9 failed\n",
      "\n",
      "\n",
      "  File \"<stdin>\", line 18, in <module>\n",
      "  File \"/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 423, in show\n",
      "    print(self._jdf.showString(n, int_truncate, vertical))\n",
      "  File \"/opt/amazon/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 111, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/opt/amazon/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o178.showString.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 4 times, most recent failure: Lost task 0.3 in stage 6.0 (TID 105) ([2600:1f18:aa1:3a19:3d2f:233:1dca:44d5] executor 11): org.apache.spark.sql.execution.QueryExecutionException: Encounter error while reading parquet files. One possible cause: Parquet column cannot be converted in the corresponding files. Details: \n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadParquetFilesError(QueryExecutionErrors.scala:577)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:308)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:204)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:954)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:183)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:133)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1474)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.parquet.io.ParquetDecodingException: Can not read value at 1 in block 0 in file s3://cse6242-nyc-trip/yellow_tripdata_2024-10.parquet\n",
      "\tat org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:254)\n",
      "\tat org.apache.parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:207)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator$$anon$1.hasNext(RecordReaderIterator.scala:61)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:204)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:299)\n",
      "\t... 19 more\n",
      "Caused by: java.lang.ClassCastException: org.apache.spark.sql.catalyst.expressions.MutableLong cannot be cast to org.apache.spark.sql.catalyst.expressions.MutableInt\n",
      "\tat org.apache.spark.sql.catalyst.expressions.SpecificInternalRow.setInt(SpecificInternalRow.scala:254)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter$RowUpdater.setInt(ParquetRowConverter.scala:179)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetPrimitiveConverter.addInt(ParquetRowConverter.scala:89)\n",
      "\tat org.apache.parquet.column.impl.ColumnReaderBase$2$3.writeValue(ColumnReaderBase.java:297)\n",
      "\tat org.apache.parquet.column.impl.ColumnReaderBase.writeCurrentValueToConverter(ColumnReaderBase.java:440)\n",
      "\tat org.apache.parquet.column.impl.ColumnReaderImpl.writeCurrentValueToConverter(ColumnReaderImpl.java:30)\n",
      "\tat org.apache.parquet.io.RecordReaderImplementation.read(RecordReaderImplementation.java:406)\n",
      "\tat org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:229)\n",
      "\t... 24 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2610)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2559)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2558)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2558)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1200)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1200)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1200)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2798)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2740)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2729)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.checkNoFailures(AdaptiveExecutor.scala:154)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.doRun(AdaptiveExecutor.scala:88)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.tryRunningAndGetFuture(AdaptiveExecutor.scala:66)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.execute(AdaptiveExecutor.scala:57)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:241)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:240)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:509)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:471)\n",
      "\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3779)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2769)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3770)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3768)\n",
      "\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2769)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2976)\n",
      "\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:289)\n",
      "\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:328)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.sql.execution.QueryExecutionException: Encounter error while reading parquet files. One possible cause: Parquet column cannot be converted in the corresponding files. Details: \n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadParquetFilesError(QueryExecutionErrors.scala:577)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:308)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:204)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:954)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:183)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:133)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1474)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: org.apache.parquet.io.ParquetDecodingException: Can not read value at 1 in block 0 in file s3://cse6242-nyc-trip/yellow_tripdata_2024-10.parquet\n",
      "\tat org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:254)\n",
      "\tat org.apache.parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:207)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator$$anon$1.hasNext(RecordReaderIterator.scala:61)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:204)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:299)\n",
      "\t... 19 more\n",
      "Caused by: java.lang.ClassCastException: org.apache.spark.sql.catalyst.expressions.MutableLong cannot be cast to org.apache.spark.sql.catalyst.expressions.MutableInt\n",
      "\tat org.apache.spark.sql.catalyst.expressions.SpecificInternalRow.setInt(SpecificInternalRow.scala:254)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter$RowUpdater.setInt(ParquetRowConverter.scala:179)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetPrimitiveConverter.addInt(ParquetRowConverter.scala:89)\n",
      "\tat org.apache.parquet.column.impl.ColumnReaderBase$2$3.writeValue(ColumnReaderBase.java:297)\n",
      "\tat org.apache.parquet.column.impl.ColumnReaderBase.writeCurrentValueToConverter(ColumnReaderBase.java:440)\n",
      "\tat org.apache.parquet.column.impl.ColumnReaderImpl.writeCurrentValueToConverter(ColumnReaderImpl.java:30)\n",
      "\tat org.apache.parquet.io.RecordReaderImplementation.read(RecordReaderImplementation.java:406)\n",
      "\tat org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:229)\n",
      "\t... 24 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add temporal dimensions with null checks\n",
    "from itertools import chain\n",
    "# 2. Temporal analysis with null handling\n",
    "# Extract date components from the correct pickup datetime column\n",
    "taxi_df = taxi_df.withColumn(\"year\", year(\"tpep_pickup_datetime\")) \\\n",
    "                 .withColumn(\"month\", month(\"tpep_pickup_datetime\")) \\\n",
    "                 .withColumn(\"day_of_week\", dayofweek(\"tpep_pickup_datetime\"))\n",
    "\n",
    "# Aggregate the data by year, month, day of week, pickup location, and drop-off location\n",
    "aggregated_df = taxi_df.groupBy(\n",
    "    \"year\",\n",
    "    \"month\",\n",
    "    \"day_of_week\",\n",
    "    \"PULocationID\",\n",
    "    \"DOLocationID\"\n",
    ").agg(count(\"*\").alias(\"num_trips\"))\n",
    "\n",
    "# Display the aggregated data\n",
    "aggregated_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation started (calculation_id=5ecb169b-6aff-4fe9-ab18-7945a8d365e3) in (session=40cb1698-5db5-4b54-871f-bb5c6f2edb87). Checking calculation status...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12a060ad1acc42369411aed4a9734a10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          |elapsed time = 00:00s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation 5ecb169b-6aff-4fe9-ab18-7945a8d365e3 failed\n",
      "\n",
      "\n",
      "  File \"<stdin>\", line 5, in <module>\n",
      "AttributeError: 'function' object has no attribute 'getOrCreate'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Disable vectorized reader to sidestep aggressive schema conversion issues\n",
    "spark.conf.set(\"spark.sql.parquet.enableVectorizedReader\", \"false\")\n",
    "\n",
    "# Read the dataset (let Spark infer the schema)\n",
    "taxi_df = spark.read.parquet(\"s3://cse6242-nyc-trip/yellow_tripdata_202*.parquet\")\n",
    "\n",
    "# Now cast the location ID columns to long if needed (or keep as int if that's acceptable)\n",
    "taxi_df = taxi_df.withColumn(\"PULocationID\", F.col(\"PULocationID\").cast(\"long\")) \\\n",
    "                 .withColumn(\"DOLocationID\", F.col(\"DOLocationID\").cast(\"long\"))\n",
    "\n",
    "# Extract date components from the pickup datetime column\n",
    "taxi_df = taxi_df.withColumn(\"year\", F.year(\"tpep_pickup_datetime\")) \\\n",
    "                 .withColumn(\"month\", F.month(\"tpep_pickup_datetime\")) \\\n",
    "                 .withColumn(\"day_of_week\", F.dayofweek(\"tpep_pickup_datetime\"))\n",
    "\n",
    "# Aggregate the data\n",
    "aggregated_df = taxi_df.groupBy(\n",
    "    \"year\",\n",
    "    \"month\",\n",
    "    \"day_of_week\",\n",
    "    \"PULocationID\",\n",
    "    \"DOLocationID\"\n",
    ").agg(F.count(\"*\").alias(\"num_trips\"))\n",
    "\n",
    "aggregated_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation started (calculation_id=06cb16b3-91ba-1643-8123-49cd5b4092a7) in (session=1ecb16b1-e1a9-baa7-f760-e157ed47fab0). Checking calculation status...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e843505b71447f28c88008d73fc8df6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          |elapsed time = 00:00s"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation 06cb16b3-91ba-1643-8123-49cd5b4092a7 failed\n",
      "\n",
      "\n",
      "  File \"<stdin>\", line 2, in <module>\n",
      "ModuleNotFoundError: No module named 'pyspark.sql.context'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.context import SQLContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType, StringType, TimestampType\n",
    "\n",
    "# Create a SparkContext.\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# Create an SQLContext using the SparkContext.\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "# Define an explicit schema that matches the parquet file schema.\n",
    "schema = StructType([\n",
    "    StructField(\"VendorID\", IntegerType(), True),\n",
    "    StructField(\"tpep_pickup_datetime\", TimestampType(), True),\n",
    "    StructField(\"tpep_dropoff_datetime\", TimestampType(), True),\n",
    "    StructField(\"passenger_count\", IntegerType(), True),\n",
    "    StructField(\"trip_distance\", DoubleType(), True),\n",
    "    StructField(\"RatecodeID\", IntegerType(), True),\n",
    "    StructField(\"store_and_fwd_flag\", StringType(), True),\n",
    "    StructField(\"PULocationID\", IntegerType(), True),  # matches INT32 in file\n",
    "    StructField(\"DOLocationID\", IntegerType(), True),   # matches INT32 in file\n",
    "    StructField(\"payment_type\", IntegerType(), True),\n",
    "    StructField(\"fare_amount\", DoubleType(), True),\n",
    "    StructField(\"extra\", DoubleType(), True),\n",
    "    StructField(\"mta_tax\", DoubleType(), True),\n",
    "    StructField(\"tip_amount\", DoubleType(), True),\n",
    "    StructField(\"tolls_amount\", DoubleType(), True),\n",
    "    StructField(\"improvement_surcharge\", DoubleType(), True),\n",
    "    StructField(\"total_amount\", DoubleType(), True),\n",
    "    StructField(\"congestion_surcharge\", DoubleType(), True),\n",
    "    StructField(\"airport_fee\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Read the dataset with the explicit schema (adjust the S3 path as needed)\n",
    "taxi_df = sqlContext.read.schema(schema).parquet(\"s3://cse6242-nyc-trip/yellow_tripdata_2024-10.parquet\")\n",
    "\n",
    "# Extract date components from the pickup datetime column.\n",
    "taxi_df = taxi_df.withColumn(\"year\", F.year(\"tpep_pickup_datetime\")) \\\n",
    "                 .withColumn(\"month\", F.month(\"tpep_pickup_datetime\")) \\\n",
    "                 .withColumn(\"day_of_week\", F.dayofweek(\"tpep_pickup_datetime\"))\n",
    "\n",
    "# Aggregate by year, month, day of week, pickup location, and drop-off location.\n",
    "aggregated_df = taxi_df.groupBy(\n",
    "    \"year\",\n",
    "    \"month\",\n",
    "    \"day_of_week\",\n",
    "    \"PULocationID\"\n",
    "    \"DOLocationID\"\n",
    ").agg(F.count(\"*\").alias(\"num_trips\"))\n",
    "\n",
    "aggregated_df.show(truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Athena PySpark",
   "language": "python",
   "name": "kepler_python_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "Python_Session",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
